{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. AR (Autoregressive) Processes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{AR}$ (autoregressive) model is simply a linear combination of $\\color{red}{p}$ lags with **normally distributed residuals**\n",
    "\n",
    "$$Y_t = \\alpha + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + \\dots + \\beta_{\\color{red}{p}} Y_{t-\\color{red}{p}} + \\epsilon_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ar_process(ar1, ar2=0, diff=0, n=500, mu=0, sigma=1, show_plot=True, return_y=False):\n",
    "    '''\n",
    "    Plot an auto-regressive time series, as well as its ACF/PACF plots\n",
    "    '''\n",
    "    X=np.arange(n)\n",
    "    y_list = []\n",
    "    y0 = 0\n",
    "    y1 = 0\n",
    "    for i in range(n):\n",
    "        # build an AR process of params (beta_1, beta_2) = (ar1, ar2)\n",
    "        # With noise epsilon(t) = Normal(mu, sigma)\n",
    "        y_new = ar1 * y1 + ar2* y0 + scipy.stats.norm.rvs(mu,sigma)\n",
    "        y0 = y1\n",
    "        y1 = y_new.copy()\n",
    "        y_list.append(y0)\n",
    "    if diff > 0:\n",
    "        for i in range(diff):\n",
    "            y_list = list(pd.Series(y_list).diff())\n",
    "        y_list = y_list[diff:]\n",
    "        X = X[diff:]\n",
    "    if show_plot:\n",
    "        fig = plt.figure(figsize=(20,10))\n",
    "        ax1 = fig.add_subplot(2,1,1)\n",
    "        ax1.set_title(f'Auto-regressive stationary TS with lagged coefs = ({ar1}, {ar2})')\n",
    "        ax2 = fig.add_subplot(2,2,3)\n",
    "        ax3 = fig.add_subplot(2,2,4)\n",
    "        ax1.plot(X,y_list)\n",
    "        plot_acf(y_list, lags=50, auto_ylims=True,ax=ax2);\n",
    "        plot_pacf(y_list, lags=50, method='ywm', auto_ylims=True, ax=ax3, color='r');\n",
    "        plt.show()\n",
    "    if return_y:\n",
    "        return y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary AR Processes\n",
    "\n",
    "üëá Try to understand the code above, then run it for $\\beta_1$ < 1, let's say 0.95 (`ar1=0.95`) and `n=200`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìÔ∏è Try to answer the following questions:\n",
    "\n",
    "1. Does the time series look stationary? Why?\n",
    "2. Do you see the AR1 term in your PACF?\n",
    "3. What is the relationship between subsequent terms in the normal ACF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answers</summary>\n",
    "\n",
    "1. The time series has an AR term &lt; 1, and will thus not show a growing trend but regress toward the mean; it is stationary \n",
    "2. The second peak in the PACF should be around 0.95 (corresponding to our AR(1) term). The first peak is <b>always</b> 1.0\n",
    "3. Each peak in the ACF should be approximately 0.95 times the peak before it; the propagation of the AR term can be seen far into the future for the ACF, but this effect has been removed in the PACF\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of Sample Size\n",
    "\n",
    "üëá Plot the same curves but with n=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùìÔ∏è Questions\n",
    "1. What changed about the ACF and PACF (*hint: look at the blue cone*)?\n",
    "2. Does this make sense to you? Remember what you know about sample sizes and confidence intervals!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answers</summary>\n",
    "\n",
    "1. The blue cones have gotten smaller, as have the insignificant peaks in the PACF; the blue cones are the confidence intervals\n",
    "2. Increased sample size will make our confidence intervals smaller (Central Limit Theorem!)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Stationary AR Process\n",
    "\n",
    "Now let's look at what happens when $\\beta_1$ >= 1.\n",
    "\n",
    "üëá Plot the time series and (P)ACF for $\\beta_1$ = 1.01 and n=200. Set `np.random.seed(1)` at the top of your cell to reproduce our results\n",
    "\n",
    "You should now see a trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìÔ∏èQuestions:\n",
    "1. Why do you see a trend?\n",
    "2. Is the time series still stationary?\n",
    "3. If not, what would you do to make it stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answers</summary>\n",
    "\n",
    "1. An AR term of 1 or more (or a combination of AR terms that add up to approximately 1) will not regress to the mean, and show a trend instead. The model is said to have \"unit root\". If your AR term is larger than 1, it will grow exponentially, as each value of Y will be bigger than the one before.\n",
    "2. The time series is not stationary anymore (it has unit root, which can be tested with a unit root test like the ADF), since it has a time-dependent mean (the mean changes over time, i.e. it has a trend).\n",
    "3. We can de-trend or difference the time series. With such exponential behavior, it is also helpful to work with the logarithm of your time series.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Behavior\n",
    "\n",
    "üëá Plot the same time series for n=500, keep the same random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìÔ∏èWhy does the time series quickly becomes exponential?\n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "Each value of Y will be its previous value multiplied by 1.01, which after a while will grow to be very large numbers. This is called exponential behavior.\n",
    "    \n",
    "üí° This is also why 1% more annual return of an investment makes a bit of a difference after a few years :)\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "üëá Keeping n=200 and increasing $\\beta_1$ slightly (say, to 1.1) should result in similar behavior. Try it (keep random seed 1)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you kept the random seed at 1, you should find that with ar1=1.1 the time series will now be exponentially decreasing.\n",
    "\n",
    "‚ùìÔ∏èWhy is that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "Some small fluctuation in the beginning made some value of y negative, which then quickly escaped to negative infinity. \n",
    "    \n",
    "üí° This type of behavior (tiny fluctuations having a huge impact) plays an important role in chaos theory and the [Butterfly effect](https://en.wikipedia.org/wiki/Butterfly_effect).\n",
    "</details>\n",
    "\n",
    "‚òùÔ∏è **Comments** \n",
    "\n",
    "This behavior is true in nature for any process with a true exponential behavior (i.e. AR coefficients larger than 1).\n",
    "\n",
    "Many processes in real life will be exponential in the long run, but seem like they only have a linear trend in the short run (like when we increased n to 500). If you clearly see exponential behavior in your time series, you can use a log-transformation to remove it!\n",
    "\n",
    "Another way to \"control\" this behavior is to add a negative AR(2) term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Competing AR Terms\n",
    "\n",
    "üëá Plot the same curve with $\\beta_1$ = 1.1 and n=200, but add an AR(2) term of $\\beta_2$ = -0.5. Keep `np.random.seed(1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about this!\n",
    "- The AR(1) term is trying to make the time series get bigger and bigger in one direction, because every y is larger than the y at t-1.\n",
    "- The AR(2) term counteracts this by subtracting a fraction of the value of y at t-2.\n",
    "\n",
    "‚òùÔ∏è The ACF should now show oscillatory behavior instead of an exponential decrease.\n",
    "\n",
    "‚òùÔ∏è The PACF should show a positive term followed by a negative term.\n",
    "\n",
    "‚ùìÔ∏èIs this TS stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "Yes, it is!\n",
    "\n",
    "The second AR term negates the exponential behavior of the first term. The time series will regress toward the mean, because the sum of AR coefficients is less than 1.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative AR Terms\n",
    "\n",
    "Having only a negative AR(1) term will give us an even stronger oscillatory behavior, without any of the \"stickiness\" of the mixed process we just created. The AR(1) term created \"stickiness\", the AR(2) term created \"oscillation\".\n",
    "\n",
    "Stickiness is the behavior of a time series staying at a given level for a period of time. This is due to the positive AR(1) coefficient.\n",
    "\n",
    "üëá Plot the graphs for `ar1 = -0.5` (no AR(2) term). Convince yourself that the behavior is strongly oscillatory but has no stickiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize:\n",
    "- $\\beta_{1} = 0$ ‚ûî white noise\n",
    "- $0 < \\beta_{1} < 1$ ‚ûî \"sticky\" time series regressing to the mean\n",
    "- $\\beta_{1} = 1$ ‚ûî random walk/linear trend\n",
    "- $\\beta_{1} > 1$ ‚ûî exponential trend\n",
    "- $\\beta_{1} < 0$ ‚ûî oscillations\n",
    "- $\\beta_{1} < -1$ ‚ûî exponential oscillations\n",
    "\n",
    "- Significant terms at higher lags indicate seasonality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive AR Terms\n",
    "\n",
    "What if we have two positive AR terms that are each smaller than 1?\n",
    "\n",
    "üëá Plot the time series with `ar1` = `ar2` = `0.5` and `n=1000` points. Set np.random.seed(1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Look at the PACF. The AR(1) term is actually close to 1 and not 0.5 as we could have expected. The PACF method is not perfect. Similarly to Linear Regression, we can have problems with interpretation of the coefficients because of collinearity, especially if the coefficients are on a similar scale.\n",
    "\n",
    "‚òùÔ∏è If you look at the ACF, it simply looks like a time series with a single AR term close to 1. There is almost no way to differentiate it from an AR(1) model.\n",
    "\n",
    "‚ùìÔ∏è Questions:\n",
    "1. Does this time series look stationary to you?\n",
    "2. What can we do to make it stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answers</summary>\n",
    "    \n",
    "1. No! It has a growing mean (trend) because the AR(1) and AR(2) terms add up to 1.\n",
    "2. We can de-trend or difference the time series.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differencing a Non-Stationary AR Time Series\n",
    "\n",
    "üëá Replot the same time series and (P)ACFs, but add diff=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìÔ∏è Questions:\n",
    "1. Does this time series look stationary now?\n",
    "2. Do you still see the AR(1) and AR(2) coefficients in the PACF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answers</summary>\n",
    "    \n",
    "1. Yes it does!\n",
    "2. The PACF shows a single negative peak of about -0.5. The ACF looks slightly oscillatory. The AR terms have been partially removed by differencing!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Think about it: \n",
    "- If each value of $y_t$ is approximately 0.5 times the value of $y_{t-1}$, removing a full value of $y_{t-1}$ from $y_t$ will result in a value of $y_t$ that is approximately $-0.5 * y_{t-1}$.\n",
    "\n",
    "- The time series now behaves like a time series with AR(1) = -0.5. This leads to oscillatory behavior in the ACF (the peaks flip signs and decrease by about 0.5 for each lag).\n",
    "\n",
    "‚òùÔ∏è Did we difference enough ([rules 1 & 2](http://people.duke.edu/~rnau/arimrule.htm) of the Box-Jenkins method)?\n",
    "- When the diff still has **positive** ACF terms, we talk about slight **under-differencing**: when modelling an ARIMA on it, we would have to account for these positive terms as AR terms and set the `p` hyperparameter accordingly.  \n",
    "\n",
    "- Here, we have **negative** ACF terms, meaning we have slightly **over-differenced** our time series. These terms can be accounted for with MA terms setting the `q` hyperparameter accordingly, as we will see in next challenge.\n",
    "\n",
    "\n",
    "\n",
    "Now let's talk about MA processes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MA (Moving-Average) Processes\n",
    "\n",
    "$\\color{blue}{MA}$ processes are a linear combination of $\\color{blue}{q}$ prediction errors $\\epsilon$ from a previous model (for instance, a linear trend, or an AR model). MA processes do not depend on previous values of y directly\n",
    "\n",
    "$$Y_t = \\mu + \\epsilon_t + \\color{blue}{\\phi_1} \\epsilon_{t-1} + \\color{blue}{\\phi_2} \\epsilon_{t-2} + \\dots + \\color{blue}{\\phi_q} \\epsilon_{t-q}$$\n",
    "\n",
    "\n",
    "In an $\\color{red}{AR}$ process, disturbances (changes to Y) can propagate infinitely far into the future\n",
    "\n",
    "> One example is sea levels: if sea levels rise by 5 cm this year, next year's sea levels will obviously depend on the new absolute sea level and not just the change in sea level.\n",
    "\n",
    "**$\\color{blue}{MA}$ disturbances have a finite lifetime and don't propagate far into the future**. They measure the effects of a limited-time duration (a \"shock\").\n",
    "\n",
    "> In the example of sea levels, we could inspect the time series that represents the _changes_ in sea levels instead of the sea levels themselves. The _change_ in sea levels could be due to some external effect that we are not measuring, that is of finite duration, for example an extreme heat wave of a few years. This could lead to a few years of __above average increase__ in sea levels (our errors $\\epsilon$). So for a few years, the values of _increase_ are correlated through some external process we don't know about. This will stop after a while, so the yearly _increase_ goes back to normal (and error $\\epsilon$ goes back to 0), but the absolute sea level stays high.\n",
    "\n",
    "‚òùÔ∏è Note that the change in sea levels can be considered as the _differenced_ time series of the original sea levels time series. As such, differencing can turn an AR model into an MA model!\n",
    "\n",
    "Another example of an MA process could be increasing sales for a limited time because of the circulation of discount codes or some special offers or discount sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We give you a random MA process generator (no need to look at the complex code for this challenge)\n",
    "def ma_process(coef_list, n=200, show_plot=True, return_y=False):\n",
    "    '''\n",
    "    Generates an MA process from prediction error normally distributed\n",
    "    '''\n",
    "\n",
    "    X = np.arange(n)\n",
    "\n",
    "    coef_list = [1]+coef_list\n",
    "    coefs = np.asarray(coef_list)\n",
    "    n_coef = len(coefs)\n",
    "\n",
    "    noise_size = n + len(coefs)\n",
    "    noise = np.random.normal(size=noise_size)\n",
    "\n",
    "    # correlating random values with its immediate neighbors\n",
    "    y_list = np.convolve(noise, coefs[::-1])[(n_coef-1):(n+n_coef-1)]\n",
    "\n",
    "    if show_plot:\n",
    "        fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "        ax1 = fig.add_subplot(2,1,1)\n",
    "        ax1.set_title(f'MA TS with lagged coefs = {coef_list}')\n",
    "\n",
    "        ax2 = fig.add_subplot(2,2,3)\n",
    "        ax3 = fig.add_subplot(2,2,4)\n",
    "\n",
    "        ax1.plot(X,y_list)\n",
    "\n",
    "        plot_acf(y_list, lags=20, auto_ylims=True, ax=ax2);\n",
    "        plot_pacf(y_list, lags=20, method='ywm', auto_ylims=True,ax=ax3, color='r');\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    if return_y:\n",
    "        return y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single MA Term\n",
    "\n",
    "üëá Plot the curves for $\\theta_1$ = 0.6 and n=200. You now have to pass the coefficients as a list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that MA processes are _always_ stationary.\n",
    "\n",
    "‚ùìÔ∏è Question: how does the ACF differ from an AR(1) process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "The effect of the MA term does NOT propagate far into the future like for an AR term. It disappears after the first peak!\n",
    "\n",
    "‚òùÔ∏è This is why we read the MA terms in the ACF and the AR terms in the PACF.\n",
    "    \n",
    "‚òùÔ∏è Sharp cut-offs in the ACF indicate MA processes in the absence of AR processes! [Rule 7](http://people.duke.edu/~rnau/arimrule.htm)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple MA Terms\n",
    "\n",
    "üëâ Now let's plot the same for three MA coefficients: [0.8, 0.2, 0.7] with `np.random.seed(2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è As you can see, the PACF has become less useful for MA processes. We usually use the ACF to count the number of significant peaks to determine the order q of the MA model.\n",
    "\n",
    "‚òùÔ∏è The size of the peaks is not very useful anymore to determine the size of the effect, but the number of significant peaks is.\n",
    "\n",
    "Now let's mix AR and MA processes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ARMA Processes\n",
    "\n",
    "Now let's try and combine the two\n",
    "\n",
    "$$Y_{t}=\\color{red}{\\beta_{1}}Y_{t-1}+\\ldots+\\color{red}{\\beta_{p}}Y_{t-p}+\\color{blue}{\\phi_{1}}\\epsilon_{t-1}\n",
    "       +\\ldots+\\color{blue}{\\phi_{q}}\\epsilon_{t-q}+\\epsilon_{t}$$\n",
    "       \n",
    "$\\color{red}{AR(p)}$ terms in red, $\\color{blue}{MA(q)}$ terms in blue.\n",
    "\n",
    "### Creating an ARMA Process\n",
    "üëâ We will create an ARMA process with two AR and two MA terms. Note that we don't commonly deal with so many terms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's choose our AR and MA terms\n",
    "ar_params = np.array([.55, -.35]) # beta\n",
    "ma_params = np.array([.65, .95]) # phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an ARMA process using statsmodels dedicated library\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "\n",
    "ar = np.r_[1, -ar_params] # add zero-lag and negate (this is how ArmaProcess needs to be coded)\n",
    "ma = np.r_[1, ma_params] # add zero-lag\n",
    "arma_process = ArmaProcess(ar, ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then generate 200 data points of data from this ARMA process for now.\n",
    "np.random.seed(1)\n",
    "y = arma_process.generate_sample(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è The sum of the AR coefficients is below 1, so it should be stationary; confirm it with an [ADF test](https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.adfuller.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the ARMA Time Series\n",
    "\n",
    "üëâ Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Plot the ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è In theory we should find\n",
    "- AR terms = [.55, -.35] on the PACF\n",
    "- MA terms = [.65, .95] on the ACF\n",
    "\n",
    "‚òùÔ∏è We don't exactly find them all: as you can see, mixing several MA and AR terms complicates things as they interact and create strange artifacts. Differencing time series with trends also often creates strange artifacts.\n",
    "\n",
    "**Optional:** try to create a non-stationary time series by setting the AR terms to add up to approximately 1. Warning: if they exceed 1 by more than just a bit, the time series will explode to infinity very quickly. Simple differencing won't help much anymore! Try to make the time series stationary by differencing and fitting an ARIMA model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model Manually\n",
    "\n",
    "For now, let's try and fit an ARIMA with 2 AR terms (as seen in the PACF) and 1 MA term (ACF), even though we know that's not the true model, since we created the model with 2 AR and 2 MA terms. This probably won't work well because the ACF now has mixed effects of several AR and MA terms.\n",
    "\n",
    "üëâ Fit an [ARIMA](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMA.html) model with p=2, d=0, q=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Inspect the AIC, the coefficients, and their p-values with `model.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an **AIC of 635.788**.\n",
    "\n",
    "‚òùÔ∏è The summary of the model seems to suggest that the MA term is not even significant (large p-value).\n",
    "\n",
    "### Removing Non-Significant Coefficients\n",
    "\n",
    "Will it get better if we remove the MA term (we want AIC to be low)?\n",
    "\n",
    "üëâ Fit an ARIMA with parameters (2, 0, 0) and see if the AIC improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìÔ∏è Did the AIC improve?\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "Not really! The AIC stayed about the same.\n",
    "</details>\n",
    "\n",
    "### Fitting the \"True\" Model\n",
    "\n",
    "Let's try to fit the \"true\" model\n",
    "\n",
    "üëâ Fit an ARIMA with parameters (2, 0, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìÔ∏è What do you notice (AIC, coefficients, and p-values)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "Suddenly both MA terms are significant and the AIC has improved quite a bit (<b>AIC = 540</b>)! \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Unfortunately, for larger numbers of mixed AR and MA terms, the rules of thumb to find hyperparameters don't work so well anymore (see rule 8 in the [list of rules](http://people.duke.edu/~rnau/arimrule.htm)).\n",
    "\n",
    "How would an automatic ARIMA fitting fare?\n",
    "\n",
    "### Auto-ARIMA\n",
    "\n",
    "üëâ We'll use [auto_arima](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html) to grid search values for p and q from 0 to 3. We won't use differencing or seasonal parameters (`d=None`, `seasonal=False`). We set `trace=True` to inspect results for different models tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pmdarima --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "\n",
    "model = pm.auto_arima(\n",
    "    y, \n",
    "    start_p=0, start_q=0,\n",
    "    max_p=3, max_q=3, # maximum p and q\n",
    "    d = None,          # no diff\n",
    "    seasonal=False\n",
    ")   # no seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and inspect the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìÔ∏è Questions\n",
    "\n",
    "1. What is the AIC value?\n",
    "2. Did it find the \"true\" number of coefficients?\n",
    "3. Are all p-values significant?\n",
    "4. Do you see the \"true\" model in the trace? Look for it and its AIC.\n",
    "5. Do other models have similar AIC values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "    \n",
    "1. We find an AIC of 539.9, which is close to what we just found for our \"true\" model\n",
    "2. No, it found a better AIC for a model with 3 AR and 2 MA coefficients\n",
    "3. The third AR coefficient does not seem to have a significant p-value, so we could decide to throw it out, which would give us back our \"true\" (2, 0, 2) model\n",
    "4. As before, the \"true\" (2, 0, 2) model has an AIC of around 540 (541 this time), which is very close to the optimal model found by auto_arima\n",
    "5. Several models tried have an AIC of around 540, of which our \"true\" model is the one with the lowest number of coefficients. When in doubt, pick the smaller model!\n",
    "\n",
    "</details>\n",
    "\n",
    "‚òùÔ∏è One of the reasons this model is hard to fit, is that we have multiple AR and MA terms, which is not very frequent with the right order of differencing to make your time series stationary. Additionally, we chose a model where the MA terms add up to more than 1, which is also uncommon if you use the right order of differencing. See rules 8-10 [here](http://people.duke.edu/~rnau/arimrule.htm).\n",
    "\n",
    "‚òùÔ∏è Because of collinearity issues, models with many parameters can get tricky to solve clearly. Always compare a few combinations of hyperparameters (p, d, q) and when in doubt, choose the smaller model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅÔ∏è You're done!\n",
    "‚ö†Ô∏è Don't forget to push the challenge to GitHub üòÑ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
